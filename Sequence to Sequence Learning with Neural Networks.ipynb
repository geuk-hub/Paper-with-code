{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMWHdTPZce+Mk3ff6Uteljc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Sequence to Sequence Learning with Neural Networks (NIPS 2014) 실습**"],"metadata":{"id":"TvbZWqkIhyQx"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1GvJ4xMpmGCT","executionInfo":{"status":"ok","timestamp":1704799671069,"user_tz":-540,"elapsed":24652,"user":{"displayName":"임병극","userId":"12873838787335237551"}},"outputId":"2562c8a6-4944-412d-e80b-7717fa3845bd"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install torchtext==0.6"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F4oNBDhMj6qA","executionInfo":{"status":"ok","timestamp":1704799679051,"user_tz":-540,"elapsed":7987,"user":{"displayName":"임병극","userId":"12873838787335237551"}},"outputId":"a9142b24-628d-4057-af1f-b8ae1abcf55e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchtext==0.6\n","  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (4.66.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (2.31.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (2.1.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (1.23.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (1.16.0)\n","Collecting sentencepiece (from torchtext==0.6)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (2023.11.17)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6) (1.3.0)\n","Installing collected packages: sentencepiece, torchtext\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.16.0\n","    Uninstalling torchtext-0.16.0:\n","      Successfully uninstalled torchtext-0.16.0\n","Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n"]}]},{"cell_type":"markdown","source":["#### **데이터 전처리(Preprocessing)**\n","- spaCy 라이브러리 : 문장의 토큰화, 태깅 등 전처리 기능을 위한 라이브러리"],"metadata":{"id":"nxMqjKpRh9b5"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ztr6U1ZthjGY","executionInfo":{"status":"ok","timestamp":1704799716787,"user_tz":-540,"elapsed":37742,"user":{"displayName":"임병극","userId":"12873838787335237551"}},"outputId":"4f303c57-5a08-4f86-a91b-d9aff27e1fa9"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-01-09 11:28:04.542875: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-01-09 11:28:04.542932: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-01-09 11:28:04.546811: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-01-09 11:28:04.557846: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-01-09 11:28:06.442590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-01-09 11:28:09.467672: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-09 11:28:09.468282: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-09 11:28:09.468555: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n","full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n","Collecting en-core-web-sm==3.6.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.11.17)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","2024-01-09 11:28:24.908961: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-01-09 11:28:24.909014: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-01-09 11:28:24.910225: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-01-09 11:28:24.917401: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-01-09 11:28:26.412125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-01-09 11:28:28.467881: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-09 11:28:28.468495: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-09 11:28:28.468727: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n","full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n","Collecting de-core-news-sm==3.6.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.6.0/de_core_news_sm-3.6.0-py3-none-any.whl (14.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.6.0) (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.10.3)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2023.11.17)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.1.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.1.3)\n","Installing collected packages: de-core-news-sm\n","Successfully installed de-core-news-sm-3.6.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('de_core_news_sm')\n"]}],"source":["!python -m spacy download en\n","!python -m spacy download de"]},{"cell_type":"code","source":["import spacy\n","\n","spacy_en = spacy.load('en_core_web_sm') # 영어 토큰화\n","spacy_de = spacy.load('de_core_news_sm') # 독일어 토큰화"],"metadata":{"id":"IEx2mGxXiWEI","executionInfo":{"status":"ok","timestamp":1704799731277,"user_tz":-540,"elapsed":14023,"user":{"displayName":"임병극","userId":"12873838787335237551"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# 간단한 토큰화 예제\n","tokenized = spacy_en.tokenizer(\"I am a graduate student.\")\n","\n","for i, token in enumerate(tokenized):\n","    print(f\"인덱스 {i}: {token.text}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rxfqq-Uic2Z","executionInfo":{"status":"ok","timestamp":1704799731277,"user_tz":-540,"elapsed":23,"user":{"displayName":"임병극","userId":"12873838787335237551"}},"outputId":"9864763b-ef06-41e6-c195-fd142c6431cd"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["인덱스 0: I\n","인덱스 1: am\n","인덱스 2: a\n","인덱스 3: graduate\n","인덱스 4: student\n","인덱스 5: .\n"]}]},{"cell_type":"code","source":["# 독일어 문장을 토큰화한 후 순서를 뒤집는 함수\n","def tokenize_de(text):\n","    return [token.text for token in spacy_de.tokenizer(text)][::-1]\n","\n","# 영어 문장을 토큰화하는 함수\n","def tokenize_en(text):\n","    return [token.text for token in spacy_en.tokenizer(text)]"],"metadata":{"id":"EE1VWIZ0inIR","executionInfo":{"status":"ok","timestamp":1704799907858,"user_tz":-540,"elapsed":294,"user":{"displayName":"임병극","userId":"12873838787335237551"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["- Field 라이브러리를 이용해 데이터셋에 대한 전처리 내용을 명시\n","- SRC : 독일어, TRC : 영어"],"metadata":{"id":"KdoJ8y_0jbbQ"}},{"cell_type":"code","source":["from torchtext.data import Field, BucketIterator\n","\n","SRC = Field(tokenize=tokenize_de, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True)\n","TRG = Field(tokenize=tokenize_en, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True)"],"metadata":{"id":"8-ln0nlujF8K","executionInfo":{"status":"ok","timestamp":1704799909901,"user_tz":-540,"elapsed":296,"user":{"displayName":"임병극","userId":"12873838787335237551"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from torchtext.datasets import Multi30k\n","\n","train_data, valid_data, test_data = Multi30k.splits(exts=(\".de\", \".en\"), fields=(SRC, TRG), root=\"/content/drive/MyDrive/Paper Review Code/data\")"],"metadata":{"id":"XINYg4XJjzbc","executionInfo":{"status":"ok","timestamp":1704799919509,"user_tz":-540,"elapsed":8159,"user":{"displayName":"임병극","userId":"12873838787335237551"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["print(f\"train data 크기: {len(train_data.examples)}개\")\n","print(f\"valid data 크기: {len(valid_data.examples)}개\")\n","print(f\"test data 크기: {len(test_data.examples)}개\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bG4_AUxdlvz3","executionInfo":{"status":"ok","timestamp":1704799919509,"user_tz":-540,"elapsed":4,"user":{"displayName":"임병극","userId":"12873838787335237551"}},"outputId":"53e20331-6094-4d54-a1d7-23a03dcf74d0"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["train data 크기: 29000개\n","valid data 크기: 1014개\n","test data 크기: 1000개\n"]}]},{"cell_type":"code","source":["# train data에서 하나를 선택해 출력\n","print(vars(train_data.examples[30])['src'])\n","print(vars(train_data.examples[30])['trg'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0gt-6uJomgQV","executionInfo":{"status":"ok","timestamp":1704799923264,"user_tz":-540,"elapsed":277,"user":{"displayName":"임병극","userId":"12873838787335237551"}},"outputId":"b4440088-4084-42ab-a940-a8dd75860f0f"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["['.', 'steht', 'urinal', 'einem', 'an', 'kaffee', 'tasse', 'einer', 'mit', 'der', ',', 'mann', 'ein']\n","['a', 'man', 'standing', 'at', 'a', 'urinal', 'with', 'a', 'coffee', 'cup', '.']\n"]}]},{"cell_type":"markdown","source":["- Field 객체의 build_vocab 메서드를 이용해 영어와 독일어의 단어 사전을 생성\n","  - 최소 2번 이상 등장한 단어만 선택"],"metadata":{"id":"2Z6ZXzVonFpP"}},{"cell_type":"code","source":["SRC.build_vocab(train_data, min_freq=2)\n","TRG.build_vocab(train_data, min_freq=2)\n","\n","print(f\"len(SRG): {len(SRC.vocab)}\")\n","print(f\"len(TRG): {len(TRG.vocab)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z6LGkkKGmvGH","executionInfo":{"status":"ok","timestamp":1704799926172,"user_tz":-540,"elapsed":440,"user":{"displayName":"임병극","userId":"12873838787335237551"}},"outputId":"d496f85c-d7ae-449d-9e92-1a8b0f71f22a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["len(SRG): 7857\n","len(TRG): 5894\n"]}]},{"cell_type":"code","source":["print(TRG.vocab.stoi[\"abcabc\"]) # 없는 단어 : 0\n","print(TRG.vocab.stoi[TRG.pad_token]) # 패딩 : 1\n","print(TRG.vocab.stoi[\"<sos>\"]) # <sos> : 2\n","print(TRG.vocab.stoi[\"<eos>\"]) # <eos> : 3\n","print(TRG.vocab.stoi[\"hello\"])\n","print(TRG.vocab.stoi[\"world\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i2u0tzlLmv0C","executionInfo":{"status":"ok","timestamp":1704799927196,"user_tz":-540,"elapsed":2,"user":{"displayName":"임병극","userId":"12873838787335237551"}},"outputId":"8f09f422-4932-480e-ba98-fdb6e9ce2284"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","2\n","3\n","4115\n","1754\n"]}]},{"cell_type":"markdown","source":["- 한 문장에 포함된 단어가 연속적으로 LSTM에 입력되어야 함\n","  - 따라서 하나의 배치에 포함된 문장들이 가지는 단어의 개수가 유사하도록 만들면 좋음\n","  - 이를 위해 BucketIterator를 사용\n","  - BucketIterator는 PyTorch의 torchtext 라이브러리에서 제공하는 데이터 로더 중 하나\n","  - 시퀀스 길이에 따라 데이터를 정렬하고 비슷한 길이의 시퀀스들을 함께 묶어주는 역할\n","  - 배치 크기 : 128\n"],"metadata":{"id":"hyvzb81knngv"}},{"cell_type":"code","source":["import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","batch_size = 128\n","\n","# 일반적인 데이터 로더의 iterator와 유사하게 사용 가능\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, valid_data, test_data),\n","    batch_size=batch_size,\n","    device=device)"],"metadata":{"id":"glS9j3OvnipN","executionInfo":{"status":"ok","timestamp":1704799930038,"user_tz":-540,"elapsed":3,"user":{"displayName":"임병극","userId":"12873838787335237551"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["for i, batch in enumerate(train_iterator):\n","    src = batch.src\n","    trg = batch.trg\n","    print(f\"첫 번째 배치 크기: {src.shape}\")\n","\n","    # 현재 배치에 있는 하나의 문장에 포함된 정보 출력\n","    for i in range(src.shape[0]):\n","        print(f\"인덱스 {i}: {src[i][0].item()}\")\n","\n","    # 첫 번째 배치만 확인\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mGkxiEvLoRvI","executionInfo":{"status":"ok","timestamp":1704799935151,"user_tz":-540,"elapsed":736,"user":{"displayName":"임병극","userId":"12873838787335237551"}},"outputId":"1dce1bf3-ec48-4454-ab09-9f85d28238da"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["첫 번째 배치 크기: torch.Size([30, 128])\n","인덱스 0: 2\n","인덱스 1: 4\n","인덱스 2: 63\n","인덱스 3: 39\n","인덱스 4: 12\n","인덱스 5: 149\n","인덱스 6: 10\n","인덱스 7: 1802\n","인덱스 8: 6\n","인덱스 9: 12\n","인덱스 10: 31\n","인덱스 11: 1125\n","인덱스 12: 46\n","인덱스 13: 14\n","인덱스 14: 11\n","인덱스 15: 26\n","인덱스 16: 70\n","인덱스 17: 5\n","인덱스 18: 3\n","인덱스 19: 1\n","인덱스 20: 1\n","인덱스 21: 1\n","인덱스 22: 1\n","인덱스 23: 1\n","인덱스 24: 1\n","인덱스 25: 1\n","인덱스 26: 1\n","인덱스 27: 1\n","인덱스 28: 1\n","인덱스 29: 1\n"]}]},{"cell_type":"markdown","source":["#### **인코더 아키텍처**\n","- 주어진 소스 문장을 context vector로 인코딩\n","- LSTM은 hidden state와 cell state를 반환\n","- 하이퍼 파라미터\n","  - input_dim : 하나의 단어에 대한 원핫 인코딩 차원\n","  - embed_dim : 임베딩 차원\n","  - hidden_dim : 히든 상태 차원\n","  - n_layers : RNN 레이어의 개수\n","  - dropout_ratio : 드롭아웃 비율"],"metadata":{"id":"zdXaOd9zo5rO"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","# 인코더 아키텍처 정의\n","class Encoder(nn.Module):\n","    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n","        super().__init__()\n","\n","        # 임베딩은 원-핫 인코딩을 특정 차원의 임베딩으로 매핑하는 레이어\n","        self.embedding = nn.Embedding(input_dim, embed_dim)\n","\n","        # LSTM 레이어\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n","\n","        # 드롭아웃\n","        self.dropout = nn.Dropout(dropout_ratio)\n","\n","    # 인코더는 소스 문장을 입력으로 받아 context vector를 반환\n","    def forward(self, src):\n","        # src : [단어 개수, 배치 크기]\n","        embedded = self.dropout(self.embedding(src))\n","        # embedded : [단어 개수, 배치 크기, 임베딩 차원]\n","\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","        # outputs : [단어 개수, 배치 크기, 히든 차원] => 현재 단어의 출력 정보\n","        # hidden : [레이어 개수, 배치 크기, 히든 차원] => 현재까지의 모든 단어의 정보\n","        # cell : [레이어 개수, 배치 크기, 히든 차원] => 현재까지의 모든 단어의 정보\n","\n","        # context vector 반환\n","        return hidden, cell"],"metadata":{"id":"_5XkJlY1oqvG","executionInfo":{"status":"ok","timestamp":1704799939699,"user_tz":-540,"elapsed":299,"user":{"displayName":"임병극","userId":"12873838787335237551"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["#### **디코더 아키텍처**\n","- 주어진 context vector를 타겟 문장으로 디코딩\n","- LSTM은 hidden state와 cell state를 반환\n","- 하이퍼 파라미터\n","  - input_dim : 하나의 단어에 대한 원-핫 인코딩 차원\n","  - embed_dim : 임베딩 차원\n","  - hidden_dim : 히든 상태 차원\n","  - n_layers : RNN 레이어 개수\n","  - dropout_ratio : 드롭아웃 비율"],"metadata":{"id":"sGOqpm0pr4GF"}},{"cell_type":"code","source":["# 디코더 아키텍처 정의\n","class Decoder(nn.Module):\n","    def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n","        super().__init__()\n","\n","        # 임베딩은 원-핫 인코딩 말고 특정 차원의 임베딩으로 매핑하는 레이어\n","        self.embedding = nn.Embedding(output_dim, embed_dim)\n","\n","        # LSTM 레이어\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n","\n","        # FC 레이어 (인코더와 다른 부분)\n","        self.output_dim = output_dim\n","        self.fc_out = nn.Linear(hidden_dim, output_dim)\n","\n","        # 드롭아웃\n","        self.dropout = nn.Dropout(dropout_ratio)\n","\n","    # 디코더는 현재까지 출력된 문장에 대한 정보를 입력으로 받아 타겟 문장을 반환\n","    def forward(self, input, hidden, cell):\n","        # input : [배치 크기] => 단어의 개수는 항상 1개\n","        # hidden : [레이어 개수, 배치 크기, 히든 차원]\n","        # cell : [레이어 개수, 배치 크기, 히든 차원]\n","        input = input.unsqueeze(0)\n","        # input : [단어 개수 = 1, 배치 크기]\n","\n","        embedded = self.dropout(self.embedding(input))\n","        # embedded : [단어 개수, 배치 크기, 임베딩 차원]\n","\n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","        # output : [단어 개수 = 1, 배치 크기, 히든 차원] => 현재 단어의 출력 정보\n","        # hidden : [레이어 개수, 배치 크기, 히든 차원] => 현재까지의 모든 단어 정보\n","        # cell : [레이어 개수, 배치 크기, 히든 차원] => 현재까지의 모든 단어 정보\n","\n","        # 단어 개수는 1개이므로 차원 제거\n","        prediction = self.fc_out(output.squeeze(0))\n","        # prediction = [배치 크기, 출력 차원]\n","\n","        # (현재 출력 단어, 현재까지의 모든 단어 정보, 현재까지의 모든 단어 정보)\n","        return prediction, hidden, cell"],"metadata":{"id":"7RyFKLY8r3WL","executionInfo":{"status":"ok","timestamp":1704799941215,"user_tz":-540,"elapsed":299,"user":{"displayName":"임병극","userId":"12873838787335237551"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["#### Seq2Seq 아키텍처\n","- 앞서 정의한 인코더와 디코더를 가지고 있는 하나의 아키텍처\n","  - 인코더 : 주어진 문장을 context vector로 인코딩\n","  - 디코더 : 주어진 context vector를 타겟 문장으로 디코딩\n","  - 단, 디코더는 한 단어씩 넣어서 한 번씩 결과를 구함\n","\n","- Teacher forcing : 디코더의 예측을 다음 입력으로 사용하지 않고, 실제 목표 출력을 다음 입력으로 사용하는 기법"],"metadata":{"id":"UyXD3VjgN42F"}},{"cell_type":"code","source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    # 학습할 때는 완전한 형태의 소스 문장, 타겟 문장, teacher_forcing_ratio를 넣기\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        # src : [단어 개수, 배치 크기]\n","        # trg : [단어 개수, 배치 크기]\n","        # 먼저 인코더를 거쳐 context vector를 추출\n","        hidden, cell = self.encoder(src)\n","\n","        # 디코더의 최종 결과를 담을 텐서 객체 만들기\n","        trg_len = trg.shape[0] # 단어 개수\n","        batch_size = trg.shape[1] # 배치 크기\n","        trg_vocab_size = self.decoder.output_dim # 출력 차원\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","\n","        # 첫 번째 입력은 항상 <sos> 토큰\n","        input = trg[0, :]\n","\n","        # 타겟 단어의 개수만큼 반복하여 디코더에 포워딩(forwarding)\n","        for t in range(1, trg_len):\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","\n","            outputs[t] = output # FC를 거쳐서 나온 현재의 출력 단어 정보\n","            top1 = output.argmax(1) # 가장 확률이 높은 단어의 인덱스 추출\n","\n","            # teacher_forcing_ratio : 학습할 때 실제 목표 출력(ground-truth)을 사용하는 비율\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            input = trg[t] if teacher_force else top1 # 현재의 출력 결과를 다음 입력에서 넣기\n","\n","        return outputs"],"metadata":{"id":"XH4FP7TROd8N","executionInfo":{"status":"ok","timestamp":1704799942257,"user_tz":-540,"elapsed":1,"user":{"displayName":"임병극","userId":"12873838787335237551"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["#### **학습(Training)**"],"metadata":{"id":"1YEBT7usP127"}},{"cell_type":"code","source":["input_dim = len(SRC.vocab)\n","output_dim = len(TRG.vocab)\n","encoder_embed_dim = 256\n","decoder_embed_dim = 256\n","hidden_dim = 512\n","n_layers = 2\n","enc_dropout_ratio = 0.5\n","dec_dropout_ratio = 0.5"],"metadata":{"id":"mqXf-FJ1P5Nr","executionInfo":{"status":"ok","timestamp":1704799943370,"user_tz":-540,"elapsed":1,"user":{"displayName":"임병극","userId":"12873838787335237551"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# 인코더와 디코더 객체 선언\n","enc = Encoder(input_dim, encoder_embed_dim, hidden_dim, n_layers, enc_dropout_ratio)\n","dec = Decoder(output_dim, decoder_embed_dim, hidden_dim, n_layers, dec_dropout_ratio)\n","\n","# Seq2Seq 객체 선언\n","model = Seq2Seq(enc, dec, device).to(device)"],"metadata":{"id":"EWaGi2SUQEnb","executionInfo":{"status":"ok","timestamp":1704799944099,"user_tz":-540,"elapsed":419,"user":{"displayName":"임병극","userId":"12873838787335237551"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["- 논문에서 설정한 것과 같이 $\\mathcal{U}(-0.08, 0.08)$의 값으로 모델 가중치 파라미터 초기화"],"metadata":{"id":"NdyxKE7pQYPG"}},{"cell_type":"code","source":["def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param.data, -0.08, 0.08)\n","\n","model.apply(init_weights)"],"metadata":{"id":"t3Fr9qZRQV5c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704799948179,"user_tz":-540,"elapsed":4,"user":{"displayName":"임병극","userId":"12873838787335237551"}},"outputId":"b2110dc2-1bf9-403c-fbe6-9bc4c496c17a"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(7857, 256)\n","    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(5894, 256)\n","    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n","    (fc_out): Linear(in_features=512, out_features=5894, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n",")"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["- 학습 및 평가 함수 정의"],"metadata":{"id":"i2GqRpPpQp-c"}},{"cell_type":"code","source":["import torch.optim as optim\n","\n","# Adam optimizer로 학습 최적화\n","optimizer = optim.Adam(model.parameters())\n","\n","# 뒷 부분의 패딩에 대해서는 값 무시\n","TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n","criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"],"metadata":{"id":"6r1WOHwbQrN_","executionInfo":{"status":"ok","timestamp":1704799950887,"user_tz":-540,"elapsed":447,"user":{"displayName":"임병극","userId":"12873838787335237551"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# 모델 학습 함수\n","def train(model, iterator, optimizer, criterion, clip):\n","    model.train()\n","    epoch_loss = 0\n","\n","    # 전체 학습 데이터를 확인\n","    for i, batch in enumerate(iterator):\n","        src = batch.src\n","        trg = batch.trg\n","\n","        optimizer.zero_grad()\n","\n","        output = model(src, trg)\n","        # output : [출력 단어 개수, 배치 크기, 출력 차원]\n","        ouput_dim = output.shape[-1]\n","\n","        # 출력 단어의 인덱스 0은 사용하지 않음\n","        output = output[1:].view(-1, output_dim)\n","        # output = [(출력 단어의 개수 - 1) * batch size, output dim]\n","        trg = trg[1:].view(-1)\n","        # trg = [(타겟 단어의 개수 - 1) * batch size]\n","\n","        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 함수 계산\n","        loss = criterion(output, trg)\n","        loss.backward() # 기울기(gradient) 계산\n","\n","        # 기울기 clipping 진행\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","        # 파라미터 업데이트\n","        optimizer.step()\n","\n","        # 전체 손실 값 계산\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"],"metadata":{"id":"KFPKUXDoQ40l","executionInfo":{"status":"ok","timestamp":1704799952654,"user_tz":-540,"elapsed":319,"user":{"displayName":"임병극","userId":"12873838787335237551"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# 모델 평가 함수\n","def evaluate(model, iterator, criterion):\n","    model.eval() # 평가 모드\n","    epoch_loss = 0\n","\n","    with torch.no_grad():\n","        # 전체 평가 데이터를 확인하며\n","        for i, batch in enumerate(iterator):\n","            src = batch.src\n","            trg = batch.trg\n","\n","            # 평가할 때 teacher forcing는 사용하지 않음\n","            output = model(src, trg, 0)\n","            # output: [출력 단어 개수, 배치 크기, 출력 차원]\n","            output_dim = output.shape[-1]\n","\n","            # 출력 단어의 인덱스 0은 사용하지 않음\n","            output = output[1:].view(-1, output_dim)\n","            # output = [(출력 단어의 개수 - 1) * batch size, output dim]\n","            trg = trg[1:].view(-1)\n","            # trg = [(타겟 단어의 개수 - 1) * batch size]\n","\n","            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n","            loss = criterion(output, trg)\n","\n","            # 전체 손실 값 계산\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"],"metadata":{"id":"VmhDfgvfRxGL","executionInfo":{"status":"ok","timestamp":1704799954389,"user_tz":-540,"elapsed":3,"user":{"displayName":"임병극","userId":"12873838787335237551"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"metadata":{"id":"KkcrPVRYTmWM","executionInfo":{"status":"ok","timestamp":1704799955104,"user_tz":-540,"elapsed":3,"user":{"displayName":"임병극","userId":"12873838787335237551"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["import time\n","import math\n","import random\n","\n","N_EPOCHS = 20\n","CLIP = 1\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    start_time = time.time() # 시작 시간 기록\n","\n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","\n","    end_time = time.time() # 종료 시간 기록\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'seq2seq.pt')\n","\n","    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n","    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rSfL1GnV1WyS","executionInfo":{"status":"ok","timestamp":1704800838971,"user_tz":-540,"elapsed":882316,"user":{"displayName":"임병극","userId":"12873838787335237551"}},"outputId":"55f08a86-02f0-4a68-f3e3-6fcdf645ead2"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Time: 0m 45s\n","\tTrain Loss: 5.067 | Train PPL: 158.644\n","\tValidation Loss: 5.015 | Validation PPL: 150.612\n","Epoch: 02 | Time: 0m 42s\n","\tTrain Loss: 4.522 | Train PPL: 92.007\n","\tValidation Loss: 4.811 | Validation PPL: 122.812\n","Epoch: 03 | Time: 0m 43s\n","\tTrain Loss: 4.213 | Train PPL: 67.557\n","\tValidation Loss: 4.545 | Validation PPL: 94.197\n","Epoch: 04 | Time: 0m 43s\n","\tTrain Loss: 3.997 | Train PPL: 54.461\n","\tValidation Loss: 4.489 | Validation PPL: 89.076\n","Epoch: 05 | Time: 0m 43s\n","\tTrain Loss: 3.843 | Train PPL: 46.652\n","\tValidation Loss: 4.358 | Validation PPL: 78.133\n","Epoch: 06 | Time: 0m 43s\n","\tTrain Loss: 3.740 | Train PPL: 42.112\n","\tValidation Loss: 4.339 | Validation PPL: 76.628\n","Epoch: 07 | Time: 0m 43s\n","\tTrain Loss: 3.609 | Train PPL: 36.911\n","\tValidation Loss: 4.267 | Validation PPL: 71.286\n","Epoch: 08 | Time: 0m 43s\n","\tTrain Loss: 3.455 | Train PPL: 31.658\n","\tValidation Loss: 4.198 | Validation PPL: 66.574\n","Epoch: 09 | Time: 0m 43s\n","\tTrain Loss: 3.343 | Train PPL: 28.293\n","\tValidation Loss: 4.095 | Validation PPL: 60.057\n","Epoch: 10 | Time: 0m 43s\n","\tTrain Loss: 3.221 | Train PPL: 25.054\n","\tValidation Loss: 4.031 | Validation PPL: 56.304\n","Epoch: 11 | Time: 0m 43s\n","\tTrain Loss: 3.124 | Train PPL: 22.736\n","\tValidation Loss: 3.952 | Validation PPL: 52.051\n","Epoch: 12 | Time: 0m 43s\n","\tTrain Loss: 3.019 | Train PPL: 20.472\n","\tValidation Loss: 3.917 | Validation PPL: 50.225\n","Epoch: 13 | Time: 0m 43s\n","\tTrain Loss: 2.930 | Train PPL: 18.732\n","\tValidation Loss: 3.886 | Validation PPL: 48.710\n","Epoch: 14 | Time: 0m 43s\n","\tTrain Loss: 2.827 | Train PPL: 16.889\n","\tValidation Loss: 3.827 | Validation PPL: 45.916\n","Epoch: 15 | Time: 0m 43s\n","\tTrain Loss: 2.756 | Train PPL: 15.742\n","\tValidation Loss: 3.739 | Validation PPL: 42.066\n","Epoch: 16 | Time: 0m 44s\n","\tTrain Loss: 2.651 | Train PPL: 14.172\n","\tValidation Loss: 3.732 | Validation PPL: 41.748\n","Epoch: 17 | Time: 0m 46s\n","\tTrain Loss: 2.599 | Train PPL: 13.444\n","\tValidation Loss: 3.765 | Validation PPL: 43.176\n","Epoch: 18 | Time: 0m 44s\n","\tTrain Loss: 2.499 | Train PPL: 12.166\n","\tValidation Loss: 3.742 | Validation PPL: 42.194\n","Epoch: 19 | Time: 0m 45s\n","\tTrain Loss: 2.438 | Train PPL: 11.447\n","\tValidation Loss: 3.717 | Validation PPL: 41.136\n","Epoch: 20 | Time: 0m 43s\n","\tTrain Loss: 2.364 | Train PPL: 10.628\n","\tValidation Loss: 3.699 | Validation PPL: 40.397\n"]}]},{"cell_type":"code","source":["# 학습된 모델 저장\n","from google.colab import files\n","\n","files.download('seq2seq.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"RUD8msII1ZKO","executionInfo":{"status":"ok","timestamp":1704800985900,"user_tz":-540,"elapsed":432,"user":{"displayName":"임병극","userId":"12873838787335237551"}},"outputId":"4a43a5aa-8b3f-4a2f-a3f7-82043e94e69f"},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_18d5b034-447b-4b24-812e-9f823ba1c6c5\", \"seq2seq.pt\", 55605128)"]},"metadata":{}}]},{"cell_type":"code","source":["model.load_state_dict(torch.load('/content/drive/MyDrive/Paper Review Code/data/seq2seq.pt'))\n","\n","test_loss = evaluate(model, test_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0AUi3_uf5tl8","executionInfo":{"status":"ok","timestamp":1704801124499,"user_tz":-540,"elapsed":2180,"user":{"displayName":"임병극","userId":"12873838787335237551"}},"outputId":"d5a50040-7c3f-4bcf-a51d-0a3b1d9756cb"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 3.714 | Test PPL: 41.019\n"]}]},{"cell_type":"code","source":["# 번역(translation) 함수\n","def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n","    model.eval() # 평가 모드\n","\n","    if isinstance(sentence, str):\n","        nlp = spacy.load('de')\n","        tokens = [token.text.lower() for token in nlp(sentence)]\n","    else:\n","        tokens = [token.lower() for token in sentence]\n","\n","    # 처음에 <sos> 토큰, 마지막에 <eos> 토큰 붙이기\n","    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n","    print(f\"전체 소스 토큰: {tokens}\")\n","\n","    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n","    print(f\"소스 문장 인덱스: {src_indexes}\")\n","\n","    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n","\n","    # 인코더(endocer)에 소스 문장을 넣어 문맥 벡터(context vector) 계산\n","    with torch.no_grad():\n","        hidden, cell = model.encoder(src_tensor)\n","\n","    # 처음에는 <sos> 토큰 하나만 가지고 있도록 하기\n","    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n","\n","    for i in range(max_len):\n","        # 이전에 출력한 단어가 현재 단어로 입력될 수 있도록\n","        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n","\n","        with torch.no_grad():\n","            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n","\n","        pred_token = output.argmax(1).item()\n","        trg_indexes.append(pred_token) # 출력 문장에 더하기\n","\n","        # <eos>를 만나는 순간 끝\n","        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n","            break\n","\n","    # 각 출력 단어 인덱스를 실제 단어로 변환\n","    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n","\n","    # 첫 번째 <sos>는 제외하고 출력 문장 반환\n","    return trg_tokens[1:]"],"metadata":{"id":"C71Fn5NF6Xa-","executionInfo":{"status":"ok","timestamp":1704801140816,"user_tz":-540,"elapsed":301,"user":{"displayName":"임병극","userId":"12873838787335237551"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["example_idx = 10\n","\n","src = vars(test_data.examples[example_idx])['src']\n","trg = vars(test_data.examples[example_idx])['trg']\n","\n","print(f'소스 문장: {src}')\n","print(f'타겟 문장: {trg}')\n","print(\"모델 출력 결과:\", \" \".join(translate_sentence(src, SRC, TRG, model, device)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gd_SeXrfCutI","executionInfo":{"status":"ok","timestamp":1704801157299,"user_tz":-540,"elapsed":301,"user":{"displayName":"임병극","userId":"12873838787335237551"}},"outputId":"1b7cbe5c-318d-4ef6-aec4-feb2ba5cfeed"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["소스 문장: ['.', 'freien', 'im', 'tag', 'schönen', 'einen', 'genießen', 'sohn', 'kleiner', 'ihr', 'und', 'mutter', 'eine']\n","타겟 문장: ['a', 'mother', 'and', 'her', 'young', 'song', 'enjoying', 'a', 'beautiful', 'day', 'outside', '.']\n","전체 소스 토큰: ['<sos>', '.', 'freien', 'im', 'tag', 'schönen', 'einen', 'genießen', 'sohn', 'kleiner', 'ihr', 'und', 'mutter', 'eine', '<eos>']\n","소스 문장 인덱스: [2, 4, 89, 20, 203, 779, 19, 566, 625, 70, 137, 10, 365, 8, 3]\n","모델 출력 결과: a father and father are enjoying a on a public day . <eos>\n"]}]},{"cell_type":"code","source":["src = tokenize_de(\"Guten Abend.\")\n","\n","print(f'소스 문장: {src}')\n","print(\"모델 출력 결과:\", \" \".join(translate_sentence(src, SRC, TRG, model, device)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sWvysPMrCwUM","executionInfo":{"status":"ok","timestamp":1704801162726,"user_tz":-540,"elapsed":316,"user":{"displayName":"임병극","userId":"12873838787335237551"}},"outputId":"e6c69b2e-e6f2-4ed8-e9d8-45fb12cdb835"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["소스 문장: ['.', 'Abend', 'Guten']\n","전체 소스 토큰: ['<sos>', '.', 'abend', 'guten', '<eos>']\n","소스 문장 인덱스: [2, 4, 1162, 3801, 3]\n","모델 출력 결과: tortillas . <eos>\n"]}]}]}